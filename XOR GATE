import numpy as np

# Define the unit step function (activation function)
def unitStep(v):
    return 1 if v >= 0 else 0

# Perceptron model to calculate output
def perceptronModel(x, w, b):
    v = np.dot(w, x) + b
    y = unitStep(v)
    return y

# Training function using perceptron learning rule
def trainPerceptron(X, Y, epochs=10000, learning_rate=0.1):
    w = np.random.rand(X.shape[1])  # Initialize weights randomly for the inputs
    b = np.random.rand(1)           # Initialize the bias randomly

    for epoch in range(epochs):
        total_error = 0
        for i in range(len(X)):
            x_i = X[i]
            target = Y[i]

            # Calculate the current output
            output = perceptronModel(x_i, w, b)

            # Calculate the error
            error = target - output
            total_error += abs(error)

            # Update weights and bias using the perceptron learning rule
            w += learning_rate * error * x_i
            b += learning_rate * error

        # Stop early if all outputs are correct
        if total_error == 0:
            break

    return w, b

# Train perceptrons for AND, OR, and NOT gates
def trainXOR():
    # Training data for AND gate
    X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])
    Y_AND = np.array([0, 0, 0, 1])
    w_AND, b_AND = trainPerceptron(X, Y_AND)

    # Training data for OR gate
    Y_OR = np.array([0, 1, 1, 1])
    w_OR, b_OR = trainPerceptron(X, Y_OR)

    # Training data for NOT gate
    X_NOT = np.array([[0], [1]])
    Y_NOT = np.array([1, 0])
    w_NOT, b_NOT = trainPerceptron(X_NOT, Y_NOT)

    return w_AND, b_AND, w_OR, b_OR, w_NOT, b_NOT

# XOR logic using the trained perceptrons for AND, OR, and NOT gates
def XOR_logicFunction(x, w_AND, b_AND, w_OR, b_OR, w_NOT, b_NOT):
    # First layer: AND and OR gates
    output_AND = perceptronModel(x, w_AND, b_AND)
    output_OR = perceptronModel(x, w_OR, b_OR)

    # Second layer: NOT gate (applied to the output of AND gate)
    output_NOT = perceptronModel(np.array([output_AND]), w_NOT, b_NOT)


    final_input = np.array([output_OR, output_NOT])
    final_output = perceptronModel(final_input, np.array([1, 1]), -1.5)
    return final_output

# Train the perceptrons
w_AND, b_AND, w_OR, b_OR, w_NOT, b_NOT = trainXOR()

# Test the trained XOR function
test_cases = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])
for test in test_cases:
    result = XOR_logicFunction(test, w_AND, b_AND, w_OR, b_OR, w_NOT, b_NOT)
    print(f"XOR({test[0]}, {test[1]}) = {result}")
     print(f"XOR({test[0]}, {test[1]}) = {result}")
