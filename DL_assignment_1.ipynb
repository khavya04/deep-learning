{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UPE1Uf_XcJ8y",
        "outputId": "eb0e58ef-6952-42f7-83c5-060068e367b6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "XOR(0, 1) = 1\n",
            "XOR(1, 1) = 0\n",
            "XOR(0, 0) = 0\n",
            "XOR(1, 0) = 1\n"
          ]
        }
      ],
      "source": [
        "#XOR GATE\n",
        "import numpy as np\n",
        "# define unit step function\n",
        "def unitStep(v):\n",
        "  if v>=0:\n",
        "    return 1\n",
        "  else:\n",
        "    return 0\n",
        "# design Perceptron Model\n",
        "def perceptronModel(x,w,b):\n",
        "  v = np.dot(w,x)+b\n",
        "  y = unitStep(v)\n",
        "  return y\n",
        "#not logic function\n",
        "#wnot = -1, bnot = 0.5\n",
        "def NOT_logicFunction(x):\n",
        "  wNOT = -1\n",
        "  bNOT = 0.5\n",
        "  return perceptronModel(x,wNOT,bNOT)\n",
        "#and logic funuction\n",
        "#here w1 =wand1 = 1, w2= wand2 = 1, band =-1.5\n",
        "def AND_logicFunction(x):\n",
        "  w = np.array([1,1])\n",
        "  bAND = -1.5\n",
        "  return perceptronModel(x,w,bAND)\n",
        "def OR_logicFunction(x):\n",
        "  w = np.array([1,1])\n",
        "  bOR = -0.5\n",
        "  return perceptronModel(x,w,bOR)\n",
        "def XOR_logicFunction(x):\n",
        "  y1 = AND_logicFunction(x)\n",
        "  y2 = OR_logicFunction(x)\n",
        "  y3 = NOT_logicFunction(y1)\n",
        "  final_x = np.array([y2,y3])\n",
        "  finalOutput = AND_logicFunction(final_x)\n",
        "  return finalOutput\n",
        "\n",
        "test1 = np.array([0,1])\n",
        "test2 = np.array([1,1])\n",
        "test3 = np.array([0,0])\n",
        "test4 = np.array([1,0])\n",
        "\n",
        "print(\"XOR({}, {}) = {}\".format(0,1,XOR_logicFunction(test1)))\n",
        "print(\"XOR({}, {}) = {}\".format(1,1,XOR_logicFunction(test2)))\n",
        "print(\"XOR({}, {}) = {}\".format(0,0,XOR_logicFunction(test3)))\n",
        "print(\"XOR({}, {}) = {}\".format(1,0,XOR_logicFunction(test4)))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Define the unit step function (activation function)\n",
        "def unitStep(v):\n",
        "    return 1 if v >= 0 else 0\n",
        "\n",
        "# Perceptron model to calculate output\n",
        "def perceptronModel(x, w, b):\n",
        "    v = np.dot(w, x) + b\n",
        "    y = unitStep(v)\n",
        "    return y\n",
        "\n",
        "# Training function using perceptron learning rule\n",
        "def trainPerceptron(X, Y, epochs=10000, learning_rate=0.1):\n",
        "    w = np.random.rand(X.shape[1])  # Initialize weights randomly for the inputs\n",
        "    b = np.random.rand(1)           # Initialize the bias randomly\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        total_error = 0\n",
        "        for i in range(len(X)):\n",
        "            x_i = X[i]\n",
        "            target = Y[i]\n",
        "\n",
        "            # Calculate the current output\n",
        "            output = perceptronModel(x_i, w, b)\n",
        "\n",
        "            # Calculate the error\n",
        "            error = target - output\n",
        "            total_error += abs(error)\n",
        "\n",
        "            # Update weights and bias using the perceptron learning rule\n",
        "            w += learning_rate * error * x_i\n",
        "            b += learning_rate * error\n",
        "\n",
        "        # Stop early if all outputs are correct\n",
        "        if total_error == 0:\n",
        "            break\n",
        "\n",
        "    return w, b\n",
        "\n",
        "# Train perceptrons for AND, OR, and NOT gates\n",
        "def trainXOR():\n",
        "    # Training data for AND gate\n",
        "    X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
        "    Y_AND = np.array([0, 0, 0, 1])\n",
        "    w_AND, b_AND = trainPerceptron(X, Y_AND)\n",
        "\n",
        "    # Training data for OR gate\n",
        "    Y_OR = np.array([0, 1, 1, 1])\n",
        "    w_OR, b_OR = trainPerceptron(X, Y_OR)\n",
        "\n",
        "    # Training data for NOT gate\n",
        "    X_NOT = np.array([[0], [1]])\n",
        "    Y_NOT = np.array([1, 0])\n",
        "    w_NOT, b_NOT = trainPerceptron(X_NOT, Y_NOT)\n",
        "\n",
        "    return w_AND, b_AND, w_OR, b_OR, w_NOT, b_NOT\n",
        "\n",
        "# XOR logic using the trained perceptrons for AND, OR, and NOT gates\n",
        "def XOR_logicFunction(x, w_AND, b_AND, w_OR, b_OR, w_NOT, b_NOT):\n",
        "    # First layer: AND and OR gates\n",
        "    output_AND = perceptronModel(x, w_AND, b_AND)\n",
        "    output_OR = perceptronModel(x, w_OR, b_OR)\n",
        "\n",
        "    # Second layer: NOT gate (applied to the output of AND gate)\n",
        "    output_NOT = perceptronModel(np.array([output_AND]), w_NOT, b_NOT)\n",
        "\n",
        "\n",
        "    final_input = np.array([output_OR, output_NOT])\n",
        "    final_output = perceptronModel(final_input, np.array([1, 1]), -1.5)\n",
        "    return final_output\n",
        "\n",
        "# Train the perceptrons\n",
        "w_AND, b_AND, w_OR, b_OR, w_NOT, b_NOT = trainXOR()\n",
        "\n",
        "# Test the trained XOR function\n",
        "test_cases = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
        "for test in test_cases:\n",
        "    result = XOR_logicFunction(test, w_AND, b_AND, w_OR, b_OR, w_NOT, b_NOT)\n",
        "    print(f\"XOR({test[0]}, {test[1]}) = {result}\")\n",
        "     print(f\"XOR({test[0]}, {test[1]}) = {result}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CWVCzpPwpr5y",
        "outputId": "34823559-2abb-4fd7-99f4-a1adc5c8a9ec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "XOR(0, 0) = 0\n",
            "XOR(0, 1) = 1\n",
            "XOR(1, 0) = 1\n",
            "XOR(1, 1) = 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers,models,optimizers\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def create_data():\n",
        "  X=np.random.randn(1000,10)\n",
        "  y=np.random.randn(1000,1)\n",
        "  return X,y\n",
        "\n",
        "def create_model():\n",
        "  model=models.Sequential([\n",
        "      layers.Dense(10,activation=\"relu\",input_shape=(10,)),\n",
        "      layers.Dense(20,activation=\"relu\"),\n",
        "      layers.Dense(1)\n",
        "  ])\n",
        "  return model\n",
        "\n",
        "def train_model_with_history(model,optimizer,X,y,batch_size,epochs,optimizer_name):\n",
        "  model.compile(optimizer=optimizer,loss='mean_squared_error')\n",
        "  history=[]\n",
        "  for epoch in range(epochs):\n",
        "    hist=model.fit(x,y,batch_size=batch_size,epocjs=1,verbose=0)\n",
        "    loss=hist.history['loss'][0]\n",
        "    history.append(loss)\n",
        "    hist = model.fit(X,y,batch_size = batch_size, epochs=1, verbose  = 0)\n",
        "    loss= hist.history['loss'][0]\n",
        "    history.append(loss)\n",
        "    print(f\"Epoch {epoch +1}\")\n"
      ],
      "metadata": {
        "id": "WClzOu1piuHO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "e1UTkfaUqG8r"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}